3
    3.1 sklearn的转化器和估计器
        3.1.1转化器
            fit_transform()
        3.1.2估计器
            1. 实例化估计器
            2. estimator.fit(x_train, y_train)计算
            3. 模型评估:
                (1)直接对比真实值和预测值
                 y_predict = estimator.predict()
                 y_test == y_predict
                (2)计算准确率
                 accuracy = estimator.score(x_test, y_test)
    3.2 KNN
    你的邻居判断出你的类别
        3.2.1 KNN API
            sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')
                n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数
                algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：‘ball_tree’将会使用 BallTree，
                ‘kd_tree’将使用 KDTree。‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)
        3.2.1. k-临近算法；
            k = 1
            容易受到异常点的影响
            如何确定谁是邻居
            计算距离：
                距离公式：
                (1)最容易理解的-----欧式距离就是坐标系中点到点的距离
                (2)曼哈顿距离-----绝对值距离
            注意：k值取的过大就容易分错
                k值过小容受到异常值的影响
        3.2.2. 案例1：鸢尾花种类预测
            (1)获取数据
            (2)数据计划分
            (3)特征工程: 标准化
            没必要降维因为就四个特征
            (4)KNN估计器流程
            (5)模型评估
        3.2.4 K-近邻总结
            优点: 简单，易于理解，易于实现，无需训练
            缺点：
                (1)必须制定K值，K值选择不当则分类精度不能保证(1.样本不平衡的时候，对稀有类别的预测准确率低。当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。2.对训练数据依赖度特别大，对训练数据的容错性太差。如果训练数据集中，有一两个数据是错误的，刚刚好又在需要分类的数值的旁边，这样就会直接导致预测的数据的不准确)
                (2)懒惰算法，对测试样本分类时计算量大内存开销大
            使用场景:小数据场景,几千～几万的样本，具体场景具体业务去测试

    3.3模型选择与优化
        3.3.1 模型选择与调优:Grid Search  API
            sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)
                对估计器的指定参数值进行详尽搜索
                estimator：估计器对象
                param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}
                cv：指定几折交叉验证
                fit：输入训练数据
                score：准确率
                结果分析：
                    bestscore:在交叉验证中验证的最好结果_
                    bestestimator：最好的参数模型
                    cvresults:每次交叉验证后的验证集准确率结果和训练集准确率结果

        3.3.1 什么是交叉验证(cross valiadation)
        3.3.2 超参数搜索-网格搜索(Grid Search)：
            通常情况下，有很多参数是需要手动指定的(如KNN中的K)，这种就叫超参数。
        3.3.3 鸢尾花案例增加K值调优
        3.3.4 案例: 预测facebook签到位置
        流程分析：
            1.获取数据
            2.数据处理
            目的：
                特征值x  目标值y
                a. 缩小数据范围
                    2 < x < 2.5
                    1.0 < y < 1.5
                b. time -> 年月日分秒
                c. 过滤签到次数少的地点
            3. 特征工程：标准化
            4. KNN算法预估器流程
            5. 模型选择与调优
            6. 模型评估

    3.4 Naive Bayes 算法
        3.4.1 NB API 
            sklearn.naive_bayes.MultinomialNB(alpha = 1.0)
                朴素贝叶斯分类
                alpha：拉普拉斯平滑系数
        3.4.1 什么是朴素贝叶斯分类方法
        3.4.2 概率基础
            联合概率、条件概率与相互独立
            条件概率：就是事件A在另外一个事件B已经发生的条件下发生的条件概率
            相互独立：
                P(A,B) = P(A)P(B) <=>  事件A与事件B相互独立
                朴素贝叶斯假设：特征与特征之间是相互独立的
            朴素贝叶斯算法：
                朴素（两事件相互独立）+贝叶斯（贝叶斯公式）
            应用场景：
                文本分类比较常见： 但此作为特征
            拉普拉斯平滑系数
        3.4.6 案例：20类新闻分类
            1)获取数据
            2）划分数据集
            3)特征工程：
                文本特征抽取
            4)朴素贝叶斯预估器流程
            5)模型评估
        3.4.7 朴素贝叶斯算法总结
            优点：
                朴素贝叶斯模型发源于古典数学理论， 有稳定的分类效率
                对缺失的数据不太敏感， 算法也比较简单， 常用于文本分类
                分类准确度高、速度快
            缺点：
                由于使用了样本属性独立的假设， 所以如果特好着呢个属性有关联时对其效果不好
        
    3.5 决策树
        3.5.1 决策树API
            决策树API
            class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)
                决策树分类器
                criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’
                max_depth:树的深度大小
                random_state:随机数种子
            其中会有些超参数：max_depth:树的深度大小
                其它超参数我们会结合随机森林讲解

        3.5.1 认识决策树
            如何高效的进行决策？
                特征的先后顺序
        3.5.2 决策树分类原理详解
        信息论基础：
            1)信息 
                小明年龄“我今年18岁” - 信息
                小华“小明明年19岁” - 不是信息
            2)信息的衡量 - 信息量 - 信息熵
            信息增益表示得知特征的X的信息之后熵的不确定性减少的程度使得类Y的信息熵减少的程度
            g(D,A) = H(D) - 条件熵H(D|A)
        3.5.7 决策树总结：
            优点：
                简单的理解和解释，树木可视化
            缺点：
                如果不设置枝叶的条件容易产生过拟合
            改进：
                剪枝cart算法(决策树API当中已经实现，随机森林参数调优有相关介绍)
                随机森林
        3.5.8 案例：titanic
        流程分析：
            特征值 目标值
            1). 获取数据
            2). 数据处理
                    缺失值处理
                    特征值转化成字典类型
            3). 准备好特征值目标值
            4). 划分好数据集
            5). 特征工程：字典特征抽取
            6). 决策树预估器流程
            7). 模型评估


    3.6 集成学习方法之随机森林
        3.6.1 RF 决策树API
        class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)
            随机森林分类器
            n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200
            criteria：string，可选（default =“gini”）分割特征的测量方法
            max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30
            max_features="auto”,每个决策树的最大特征数量
                If "auto", then max_features=sqrt(n_features).
                If "sqrt", then max_features=sqrt(n_features) (same as "auto").
                If "log2", then max_features=log2(n_features).
                If None, then max_features=n_features.
            bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样
            min_samples_split:节点划分最少样本数
            min_samples_leaf:叶子节点的最小样本数
        超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf

        3.6.1 什么是集成学习方法
        3.6.2 什么是随机森林：随机森林，顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输 入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。随机森林由决策树组成，决策树实际上是将空间用超平面进行划分的一种方法，每次分割的时候，都将当前的空间一分为二。
            随机
            森林：包含多个决策树的分类器
        3.6.3 随机森林原理过程
        训练集： 特征值 目标值
            随机森林的两个随机：
                训练集随机 - N个样本中随机又放回的抽样n个
                    bootstrp：随机又放回抽样
                    [1,2,3,4,5]
                    新的树训练集
                    [2,2,3,1,5]
                特征随机 - 从M个特征中随机抽取m个特征
                    M >> m
                    降维
        3.6.6 总结
        在当前所有算法中具有较好的准确率
        能够有效地运行在大数据集上
        处理具有高为特征的输入样本，而且不需要降维
    



                

        







            


    


                





            
    

            


            













